{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c855360-21f1-4d7a-9d12-2718534f1676",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implementation of WGAN with gradient penalty\n",
    "\"\"\"\n",
    "import pytorch_lightning as L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Construct residual block\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, num_channels, res_rate=0.3):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.res_rate = res_rate\n",
    "        \n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv1d(num_channels, num_channels, kernel_size=5, stride=1, padding='same'),\n",
    "            # nn.BatchNorm1d(num_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv1d(num_channels, num_channels, kernel_size=5, stride=1, padding='same'),\n",
    "            # nn.BatchNorm1d(num_channels),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.uniform_(m.weight, \n",
    "                                 -torch.sqrt(torch.tensor(3.)) * torch.sqrt(4. / torch.tensor(5 * self.num_channels + 5 * self.num_channels)), \n",
    "                                 torch.sqrt(torch.tensor(3.)) * torch.sqrt(4. / torch.tensor(5 * self.num_channels + 5 * self.num_channels)))\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.conv_block1(x)\n",
    "        res = self.conv_block2(res)\n",
    "\n",
    "        return x + self.res_rate * res\n",
    "\n",
    "\n",
    "# Construct generator with res blocks\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, num_channels, seq_len, res_rate, vocab_size=4, res_layers=5):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.num_channels = num_channels\n",
    "        \n",
    "        # Linear layer to transform the latent vector\n",
    "        self.linear = nn.Linear(latent_dim, seq_len * num_channels)\n",
    "        \n",
    "        self.res_blocks = nn.ModuleList([ResBlock(num_channels, res_rate) for _ in range(res_layers)])\n",
    "        \n",
    "        self.conv = nn.Conv1d(num_channels, vocab_size, kernel_size=1, stride=1, padding='same')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.view(-1, self.num_channels, self.seq_len)\n",
    "        \n",
    "        for res_block in self.res_blocks:\n",
    "            x = res_block(x)\n",
    "            \n",
    "        x = self.conv(x)\n",
    "        \n",
    "        return F.softmax(x, dim=1)\n",
    "        \n",
    "    \n",
    "# Construct Critic(discriminator)\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, num_channels, seq_len, vocab_size, res_rate=0.3, res_layers=5):\n",
    "        super(Critic, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "        # Initial convolution layer\n",
    "        self.conv1 = nn.Conv1d(vocab_size, num_channels, kernel_size=1, stride=1, padding='same')\n",
    "\n",
    "        # Residual blocks\n",
    "        self.res_blocks = nn.ModuleList([ResBlock(num_channels, res_rate) for _ in range(res_layers)])\n",
    "\n",
    "        # Final linear layer for scoring\n",
    "        self.fc = nn.Linear(seq_len * num_channels, 1)\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.uniform_(m.weight, \n",
    "                                 -torch.sqrt(torch.tensor(3.)) * torch.sqrt(2. / torch.tensor(m.weight.size(0) + m.weight.size(1))), \n",
    "                                  torch.sqrt(torch.tensor(3.)) * torch.sqrt(2. / torch.tensor(m.weight.size(0) + m.weight.size(1))))\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        for res_block in self.res_blocks:\n",
    "            x = res_block(x)\n",
    "\n",
    "        # Flatten the output for the linear layer\n",
    "        x = x.view(-1, self.seq_len * self.num_channels)\n",
    "\n",
    "        score = self.fc(x)\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "feef0002-c01e-4b20-bd88-f082e39cbd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Critic(num_channels=100, seq_len=10, vocab_size=4, res_layers=5)\n",
    "input_tensor = torch.randn(32, 4, 10)  # Example input tensor with batch size 32\n",
    "output_scores = discriminator(input_tensor)\n",
    "a = output_scores.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dea9e1e7-1515-461b-be2d-b83c4a365bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "39a0ef17-aaaf-4562-a0ba-47519f1396dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2502, 0.2420, 0.2358, 0.1445, 0.0912, 0.2917, 0.0784, 0.2197, 0.4026,\n",
       "         0.2060],\n",
       "        [0.1809, 0.1357, 0.1790, 0.0696, 0.2154, 0.1293, 0.1503, 0.2322, 0.1444,\n",
       "         0.2056],\n",
       "        [0.1902, 0.1041, 0.2549, 0.1560, 0.0709, 0.0888, 0.0871, 0.2930, 0.1521,\n",
       "         0.3276],\n",
       "        [0.3786, 0.5182, 0.3303, 0.6299, 0.6225, 0.4902, 0.6843, 0.2551, 0.3009,\n",
       "         0.2608]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = Generator(latent_dim=100, num_channels=100, seq_len=10, vocab_size=4, res_layers=5, res_rate=0.3)\n",
    "latent_vector = torch.randn(32, 100)  # Example latent vector\n",
    "output = generator(latent_vector)\n",
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "55685296-fd33-40f2-aaf6-05e3fc0f68ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from WGAN_Model import WGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "98a5def9-b307-4199-b4fe-5988bf3924ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WGAN(\n",
       "  (generator): Generator(\n",
       "    (linear): Linear(in_features=100, out_features=1000, bias=True)\n",
       "    (res_blocks): ModuleList(\n",
       "      (0-4): 5 x ResBlock(\n",
       "        (conv_block1): Sequential(\n",
       "          (0): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=same)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "        (conv_block2): Sequential(\n",
       "          (0): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=same)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv): Conv1d(100, 4, kernel_size=(1,), stride=(1,), padding=same)\n",
       "  )\n",
       "  (critic): Critic(\n",
       "    (conv1): Conv1d(4, 100, kernel_size=(1,), stride=(1,), padding=same)\n",
       "    (res_blocks): ModuleList(\n",
       "      (0-4): 5 x ResBlock(\n",
       "        (conv_block1): Sequential(\n",
       "          (0): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=same)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "        (conv_block2): Sequential(\n",
       "          (0): Conv1d(100, 100, kernel_size=(5,), stride=(1,), padding=same)\n",
       "          (1): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc): Linear(in_features=1000, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WGAN(seq_len=10, vocab_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c077892-32d1-4f99-901a-f845dfd35f10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
